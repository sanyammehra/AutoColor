{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_dir', default='cs231n/datasets/coco-animals/train')\n",
    "parser.add_argument('--val_dir', default='cs231n/datasets/coco-animals/val')\n",
    "parser.add_argument('--model_path', default='cs231n/datasets/vgg_16.ckpt', type=str)\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--num_workers', default=4, type=int)\n",
    "parser.add_argument('--num_epochs1', default=10, type=int)\n",
    "parser.add_argument('--num_epochs2', default=10, type=int)\n",
    "parser.add_argument('--learning_rate1', default=1e-3, type=float)\n",
    "parser.add_argument('--learning_rate2', default=1e-5, type=float)\n",
    "parser.add_argument('--dropout_keep_prob', default=0.5, type=float)\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float)\n",
    "\n",
    "VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "\n",
    "train_dir = 'cs231n/datasets/coco-animals/train'\n",
    "val_dir = 'cs231n/datasets/coco-animals/val'\n",
    "model_path = 'cs231n/datasets/vgg_16.ckpt'\n",
    "num_workers = 4\n",
    "batch_size = 32\n",
    "num_epochs1 = 1\n",
    "num_epochs2 = 1\n",
    "learning_rate1 = 1e-3\n",
    "learning_rate2 = 1e-5\n",
    "dropout_keep_prob = 0.5\n",
    "weight_decay = 5e-4\n",
    "train_filenames, train_labels1 = list_images(train_dir)\n",
    "val_filenames, val_labels = list_images(val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_images(directory):\n",
    "    \"\"\"\n",
    "    Get all the images and labels in directory/label/*.jpg\n",
    "    \"\"\"\n",
    "    labels = os.listdir(directory)\n",
    "    files_and_labels = []\n",
    "    for label in labels:\n",
    "        for f in os.listdir(os.path.join(directory, label)):\n",
    "            files_and_labels.append((os.path.join(directory, label, f), label))\n",
    "\n",
    "    filenames, labels = zip(*files_and_labels)\n",
    "    filenames = list(filenames)\n",
    "    labels = list(labels)\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    label_to_int = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_int[label] = i\n",
    "\n",
    "    labels = [label_to_int[l] for l in labels]\n",
    "\n",
    "    return filenames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_accuracy(sess, correct_prediction, is_training, dataset_init_op):\n",
    "    \"\"\"\n",
    "    Check the accuracy of the model on either train or val (depending on dataset_init_op).\n",
    "    \"\"\"\n",
    "    # Initialize the correct dataset\n",
    "    sess.run(dataset_init_op)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    while True:\n",
    "        try:\n",
    "            correct_pred = sess.run(correct_prediction, {is_training: False})\n",
    "            num_correct += correct_pred.sum()\n",
    "            num_samples += correct_pred.shape[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    # Return the fraction of datapoints that were correctly classified\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from cs231n/datasets/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from cs231n/datasets/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "Train accuracy: 0.567500\n",
      "Val accuracy: 0.595000\n",
      "\n",
      "Starting epoch 1 / 1\n",
      "Train accuracy: 0.598750\n",
      "Val accuracy: 0.615000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(train_labels1))\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# In TensorFlow, you first want to define the computation graph with all the\n",
    "# necessary operations: loss, training op, accuracy...\n",
    "# Any tensor created in the `graph.as_default()` scope will be part of `graph`\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "        # Standard preprocessing for VGG on ImageNet taken from here:\n",
    "        # https://github.com/tensorflow/models/blob/master/slim/preprocessing/vgg_preprocessing.py\n",
    "        # Also see the VGG paper for more details: https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "        # Preprocessing (for both training and validation):\n",
    "        # (1) Decode the image from jpg format\n",
    "        # (2) Resize the image so its smaller side is 256 pixels long\n",
    "        def _parse_function(filename, label):\n",
    "            image_string = tf.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)          # (1)\n",
    "            image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "            smallest_side = 256.0\n",
    "            height, width = tf.shape(image)[0], tf.shape(image)[1]\n",
    "            height = tf.to_float(height)\n",
    "            width = tf.to_float(width)\n",
    "\n",
    "            scale = tf.cond(tf.greater(height, width),\n",
    "                            lambda: smallest_side / width,\n",
    "                            lambda: smallest_side / height)\n",
    "            new_height = tf.to_int32(height * scale)\n",
    "            new_width = tf.to_int32(width * scale)\n",
    "\n",
    "            resized_image = tf.image.resize_images(image, [new_height, new_width])  # (2)\n",
    "            return resized_image, label\n",
    "\n",
    "        # Preprocessing (for training)\n",
    "        # (3) Take a random 224x224 crop to the scaled image\n",
    "        # (4) Horizontally flip the image with probability 1/2\n",
    "        # (5) Substract the per color mean `VGG_MEAN`\n",
    "        # Note: we don't normalize the data here, as VGG was trained without normalization\n",
    "        def training_preprocess(image, label):\n",
    "            crop_image = tf.random_crop(image, [224, 224, 3])                       # (3)\n",
    "            flip_image = tf.image.random_flip_left_right(crop_image)                # (4)\n",
    "\n",
    "            means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "            centered_image = flip_image - means                                     # (5)\n",
    "\n",
    "            return centered_image, label\n",
    "\n",
    "        # Preprocessing (for validation)\n",
    "        # (3) Take a central 224x224 crop to the scaled image\n",
    "        # (4) Substract the per color mean `VGG_MEAN`\n",
    "        # Note: we don't normalize the data here, as VGG was trained without normalization\n",
    "        def val_preprocess(image, label):\n",
    "            crop_image = tf.image.resize_image_with_crop_or_pad(image, 224, 224)    # (3)\n",
    "\n",
    "            means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "            centered_image = crop_image - means                                     # (4)\n",
    "\n",
    "            return centered_image, label\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # DATASET CREATION using tf.contrib.data.Dataset\n",
    "        # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\n",
    "\n",
    "        # The tf.contrib.data.Dataset framework uses queues in the background to feed in\n",
    "        # data to the model.\n",
    "        # We initialize the dataset with a list of filenames and labels, and then apply\n",
    "        # the preprocessing functions described above.\n",
    "        # Behind the scenes, queues will load the filenames, preprocess them with multiple\n",
    "        # threads and apply the preprocessing in parallel, and then batch the data\n",
    "\n",
    "        # Training dataset\n",
    "        train_filenames = tf.constant(train_filenames)\n",
    "        train_labels = tf.constant(train_labels1)\n",
    "        train_dataset = tf.contrib.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
    "        train_dataset = train_dataset.map(_parse_function,\n",
    "            num_threads=num_workers, output_buffer_size=batch_size)\n",
    "        train_dataset = train_dataset.map(training_preprocess,\n",
    "            num_threads=num_workers, output_buffer_size=batch_size)\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000)  # don't forget to shuffle\n",
    "        batched_train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "        # Validation dataset\n",
    "        val_filenames = tf.constant(val_filenames)\n",
    "        val_labels = tf.constant(val_labels)\n",
    "        val_dataset = tf.contrib.data.Dataset.from_tensor_slices((val_filenames, val_labels))\n",
    "        val_dataset = val_dataset.map(_parse_function,\n",
    "            num_threads=num_workers, output_buffer_size=batch_size)\n",
    "        val_dataset = val_dataset.map(val_preprocess,\n",
    "            num_threads=num_workers, output_buffer_size=batch_size)\n",
    "        batched_val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "        # Now we define an iterator that can operator on either dataset.\n",
    "        # The iterator can be reinitialized by calling:\n",
    "        #     - sess.run(train_init_op) for 1 epoch on the training set\n",
    "        #     - sess.run(val_init_op)   for 1 epoch on the valiation set\n",
    "        # Once this is done, we don't need to feed any value for images and labels\n",
    "        # as they are automatically pulled out from the iterator queues.\n",
    "\n",
    "        # A reinitializable iterator is defined by its structure. We could use the\n",
    "        # `output_types` and `output_shapes` properties of either `train_dataset`\n",
    "        # or `validation_dataset` here, because they are compatible.\n",
    "        iterator = tf.contrib.data.Iterator.from_structure(batched_train_dataset.output_types,\n",
    "                                                           batched_train_dataset.output_shapes)\n",
    "        images, labels = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(batched_train_dataset)\n",
    "        val_init_op = iterator.make_initializer(batched_val_dataset)\n",
    "\n",
    "        # Indicates whether we are in training or in test mode\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Now that we have set up the data, it's time to set up the model.\n",
    "        # For this example, we'll use VGG-16 pretrained on ImageNet. We will remove the\n",
    "        # last fully connected layer (fc8) and replace it with our own, with an\n",
    "        # output size num_classes=8\n",
    "        # We will first train the last layer for a few epochs.\n",
    "        # Then we will train the entire model on our dataset for a few epochs.\n",
    "\n",
    "        # Get the pretrained model, specifying the num_classes argument to create a new\n",
    "        # fully connected replacing the last one, called \"vgg_16/fc8\"\n",
    "        # Each model has a different architecture, so \"vgg_16/fc8\" will change in another model.\n",
    "        # Here, logits gives us directly the predicted scores we wanted from the images.\n",
    "        # We pass a scope to initialize \"vgg_16/fc8\" weights with he_initializer\n",
    "        vgg = tf.contrib.slim.nets.vgg\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=weight_decay)):\n",
    "            logits, layers = vgg.vgg_16(images, num_classes=num_classes, is_training=is_training,\n",
    "                                   dropout_keep_prob=dropout_keep_prob)\n",
    "\n",
    "        # Specify where the model checkpoint is (pretrained weights).\n",
    "        model_path = model_path\n",
    "        assert(os.path.isfile(model_path))\n",
    "        \n",
    "        # Restore only the layers up to fc7 (included)\n",
    "        # Calling function `init_fn(sess)` will load all the pretrained weights.\n",
    "        variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['vgg_16/fc8'])\n",
    "        init_fn = tf.contrib.framework.assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
    "\n",
    "        # Initialization operation from scratch for the new \"fc8\" layers\n",
    "        # `get_variables` will only return the variables whose name starts with the given pattern\n",
    "        fc8_variables = tf.contrib.framework.get_variables('vgg_16/fc8')\n",
    "        fc8_init = tf.variables_initializer(fc8_variables)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Using tf.losses, any loss is added to the tf.GraphKeys.LOSSES collection\n",
    "        # We can then call the total loss easily\n",
    "        tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        loss = tf.losses.get_total_loss()\n",
    "\n",
    "        # First we want to train only the reinitialized last layer fc8 for a few epochs.\n",
    "        # We run minimize the loss only with respect to the fc8 variables (weight and bias).\n",
    "        fc8_optimizer = tf.train.GradientDescentOptimizer(learning_rate1)\n",
    "        fc8_train_op = fc8_optimizer.minimize(loss, var_list=fc8_variables)\n",
    "\n",
    "        # Then we want to finetune the entire model for a few epochs.\n",
    "        # We run minimize the loss only with respect to all the variables.\n",
    "        full_optimizer = tf.train.GradientDescentOptimizer(learning_rate2)\n",
    "        full_train_op = full_optimizer.minimize(loss)\n",
    "\n",
    "        # Evaluation metrics\n",
    "        prediction = tf.to_int32(tf.argmax(logits, 1))\n",
    "        correct_prediction = tf.equal(prediction, labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        tf.get_default_graph().finalize()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Now that we have built the graph and finalized it, we define the session.\n",
    "# The session is the interface to *run* the computational graph.\n",
    "# We can call our training operations with `sess.run(train_op)` for instance\n",
    "#with tf.Session(graph=graph) as sess:\n",
    "sess = tf.Session(graph=graph)\n",
    "init_fn(sess)  # load the pretrained weights\n",
    "sess.run(fc8_init)  # initialize the new fc8 layer\n",
    "        \n",
    "# Update only the last layer for a few epochs.\n",
    "for epoch in range(num_epochs1):\n",
    "# Run an epoch over the training data.\n",
    "    print('Starting epoch %d / %d' % (epoch + 1, num_epochs1))\n",
    "    # Here we initialize the iterator with the training set.\n",
    "    # This means that we can go through an entire epoch until the iterator becomes empty.\n",
    "    sess.run(train_init_op)\n",
    "    while True:\n",
    "        try:\n",
    "            _ = sess.run(fc8_train_op, {is_training: True})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "             break\n",
    "\n",
    "    # Check accuracy on the train and val sets every epoch.\n",
    "    train_acc = check_accuracy(sess, correct_prediction, is_training, train_init_op)\n",
    "    val_acc = check_accuracy(sess, correct_prediction, is_training, val_init_op)\n",
    "    print('Train accuracy: %f' % train_acc)\n",
    "    print('Val accuracy: %f\\n' % val_acc)\n",
    "\n",
    "  \n",
    "    # Train the entire model for a few more epochs, continuing with the *same* weights.\n",
    "    # for epoch in range(num_epochs2):\n",
    "    print('Starting epoch %d / %d' % (epoch + 1, num_epochs1))\n",
    "    sess.run(train_init_op)\n",
    "    while True:\n",
    "        try:\n",
    "            _ = sess.run(full_train_op, {is_training: True})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    # Check accuracy on the train and val sets every epoch\n",
    "    train_acc = check_accuracy(sess, correct_prediction, is_training, train_init_op)\n",
    "    val_acc = check_accuracy(sess, correct_prediction, is_training, val_init_op)\n",
    "    print('Train accuracy: %f' % train_acc)\n",
    "    print('Val accuracy: %f\\n' % val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-de5f273167cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../models/VGG16_COCO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1159\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1162\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m       self.saver_def = self._builder.build(\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"../models/VGG16_COCO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver() \n",
    "saver.restore(sess,'../models/models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
